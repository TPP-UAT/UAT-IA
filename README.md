# ðŸ“„ UAT-IA

## Requisitos:

- Python >= 3.11  
- Python-venv  
- Docker  
- (Soporte GPU) nvidia-container-toolkit

## ðŸ§  Â¿QuÃ© hace este repositorio?

Tiene dos opciones que sirven para poder generar los modelos de NLP, necesarios para predecir los tÃ©rminos claves en los artÃ­culos de astronomÃ­a. Estas opciones son:

```
- generate
- train
```

## ðŸ› ï¸ InstalaciÃ³n:

Para usar esta app, primero necesitÃ¡s instalar [Docker](https://docs.docker.com/engine/install/ubuntu/)

Para la compatibilidad con GPU, tambiÃ©n necesitÃ¡s instalar el [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)

Para instalar Python, podÃ©s usar los siguientes comandos:

```bash
sudo apt install python3.11
sudo apt install python3.11-venv
```

## â–¶ï¸ EjecuciÃ³n

Para poder utilizar las opciones mencionadas previamente, es necesario crear un archivo `.env` y agregar las variables de entorno:

```
MODE=generate/train
DB_URL=url_de_la_base_de_datos
```

DespuÃ©s, tenÃ©s que ejecutar el archivo docker compose. Esto se logra con el siguiente comando:

```bash
COMPOSE_PROFILES=gpu docker compose up --build
```

Si querÃ©s ejecutarlo con capacidades de GPU, o:

```bash
COMPOSE_PROFILES=non_gpu docker compose up --build
```

Si no.

Este archivo docker compose utiliza un Dockerfile que instala todas las dependencias desde el archivo `requierements.txt` dentro del contenedor.

## OpciÃ³n generate

_Nota_: No es necesario usar capacidades de GPU para esta opciÃ³n.

Para usar esta opciÃ³n, necesitÃ¡s crear una carpeta `PDFs` dentro de la carpeta `data`. AhÃ­ es donde deben estar todos los artÃ­culos (archivos en formato PDF).

Esto recuperarÃ¡ toda la informaciÃ³n necesaria de cada artÃ­culo y la guardarÃ¡ en una base de datos.

Si los artÃ­culos estÃ¡n dentro de subcarpetas, necesitÃ¡s ejecutar el archivo `move_files.py`. Ese script mueve todos los archivos desde subcarpetas a la carpeta `PDFs`.

Para exportar la informaciÃ³n generada, debÃ©s crear un archivo de volcado (dump). Esto se puede lograr ejecutando en una terminal (con el contenedor activo):

```bash
docker exec -t postgres_db pg_dump -U user -d UAT_IA -t files -t keywords > dump.sql
```

Para importar un archivo dump, colocÃ¡ el archivo dump en el directorio raÃ­z y ejecutÃ¡ los siguientes comandos:

```bash
docker cp dump.sql postgres_db:/dump.sql
docker exec -i postgres_db psql -U user -d UAT_IA -f /dump.sql
```

NOTA: Puede suceder que una palabra clave no tenga archivos para una persona, pero sÃ­ para otra. Entonces, al importar el archivo dump, podÃ©s ejecutar:

```
DELETE FROM keywords k
WHERE file_id IS NULL
  AND EXISTS (
    SELECT 1
    FROM keywords k2
    WHERE k2.keyword_id = k.keyword_id
      AND k2.file_id IS NOT NULL
);
```

Todos los procesos realizados se almacenan en un archivo de log, que se encuentra en `logs/file_generation.log`.

## OpciÃ³n train

Esta opciÃ³n se encarga de entrenar todos los modelos de NLP, en base a los archivos almacenados en la base de datos.

Los modelos generados los almecena en la carpeta raÃ­z del proyecto: 

```
./models/
â”œâ”€â”€ abstract/
â”‚   â”œâ”€â”€ 102
â”‚   â””â”€â”€ 104
â”œâ”€â”€ summarize /
â”‚   â””â”€â”€ ...
```

_Nota_: En el archivo `.gitignore` se ignora esta carpeta, por lo tanto es necesario copiarla y pegarla en el proyecto `predictions-api` para que resulte Ãºtil.

Para poder seleccionar que mÃ©todo de entrenamiento se desea realizar, es necesario modificar el archivo `trainer.py`, seleccionando cual de ellos en el siguiente array (Si se desea entrenar momentÃ¡neamente uno solo de los dos, se debe comentar el que no se quiere utilizar):

```
self.input_creators = [
    AbstractInputCreator(database)
    SummarizeInputCreator(database)
]
```

Todos los procesos realizados se almacenan en un archivo de log, que se encuentra en `logs/trainer.log`.
